{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ETL Pipeline Preparation\n",
    "Follow the instructions below to help you create your ETL pipeline.\n",
    "### 1. Import libraries and load datasets.\n",
    "- Import Python libraries\n",
    "- Load `messages.csv` into a dataframe and inspect the first few lines.\n",
    "- Load `categories.csv` into a dataframe and inspect the first few lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\markf\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# import libraries\n",
    "import pandas as pd\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from sqlalchemy import create_engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load messages dataset\n",
    "def load_messages(file_name):\n",
    "    \n",
    "    return pd.read_csv(file_name, index_col='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load categories dataset\n",
    "def load_categories(file_name):\n",
    "    \n",
    "    return pd.read_csv(file_name, index_col='id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Split `categories` into separate category columns.\n",
    "- Split the values in the `categories` column on the `;` character so that each value becomes a separate column. You'll find [this method](https://pandas.pydata.org/pandas-docs/version/0.23/generated/pandas.Series.str.split.html) very helpful! Make sure to set `expand=True`.\n",
    "- Use the first row of categories dataframe to create column names for the categories data.\n",
    "- Rename columns of `categories` with new column names.\n",
    "\n",
    "#### 2.1. Convert category values to just numbers 0 or 1.\n",
    "- Iterate through the category columns in df to keep only the last character of each string (the 1 or 0). For example, `related-0` becomes `0`, `related-1` becomes `1`. Convert the string to a numeric value.\n",
    "- You can perform [normal string actions on Pandas Series](https://pandas.pydata.org/pandas-docs/stable/text.html#indexing-with-str), like indexing, by including `.str` after the Series. You may need to first convert the Series to be of type string, which you can do with `astype(str)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dataframe of the 36 individual category columns\n",
    "def expand_categories(df, column_name, col_sep=';', value_sep='-'):\n",
    "\n",
    "    # create a dataframe of the 36 individual category columns\n",
    "    categories = df_cat[column_name].str.split(col_sep, expand=True)\n",
    "    \n",
    "    # select the first row of the categories dataframe\n",
    "    row = categories.iloc[0, :]\n",
    "    \n",
    "    # use this row to extract a list of new column names for categories.\n",
    "    categories.columns = row.str.split(value_sep, expand=True)[0]\n",
    "    \n",
    "    for column in categories:\n",
    "        # set each value to be the last character of the string\n",
    "        categories[column] = categories[column].str.split(value_sep, expand=True)[1]\n",
    "\n",
    "        # convert column from string to numeric\n",
    "        categories[column] = pd.to_numeric(categories[column])\n",
    "    \n",
    "    return categories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Merge datasets.\n",
    "- Merge the messages and categories datasets using the common id\n",
    "- Assign this combined dataset to `df`, which will be cleaned in the following steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge datasets\n",
    "def merge_msg_cat(df_msg, df_cat):\n",
    "\n",
    "    df_merge = df_msg.merge(df_cat, left_index=True, right_index=True, how='inner')\n",
    "    df_merge = df_merge.reset_index() # After merge on id, use a reliable unique index\n",
    "    return df_merge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Clean the data\n",
    "- Check how many duplicates are in this dataset.\n",
    "- Drop columns not used in the machine learning pipeline.\n",
    "- Drop repeated rows (keeping the first occurrence)\n",
    "- Drop rows with same values in the independent var (remove all).\n",
    "- Replace incoherent values in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(df, drop_columns, independent_vars, replacements):\n",
    "    \n",
    "    # Drop columns that shouldn't be read in the machine learning pipeline.\n",
    "    print(\"Dropping columns:\", drop_columns)\n",
    "    df_clean = df.drop(columns=drop_columns)\n",
    "    \n",
    "    # Drop identical rows\n",
    "    df_clean = df_clean.drop_duplicates()\n",
    "    \n",
    "    # Drop rows with same predictor values with different predicted values\n",
    "    df_clean = df_clean.drop_duplicates(subset=independent_vars, keep=False)\n",
    "    \n",
    "    df_clean = df_clean.dropna()\n",
    "    \n",
    "    # Replace values in columns\n",
    "    for col, rep in replacements:\n",
    "        df_clean[col] = df_clean[col].replace(rep[0], rep[1])\n",
    "    return df_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Save the clean dataset into an sqlite database.\n",
    "\n",
    "Connect to the database and save the data. The data will be stored into a table. Keep this table name to read the data and create tha Machine Learning Pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save(df, database, table_name):\n",
    "    engine = create_engine('sqlite:///'+database)\n",
    "    df.to_sql(table_name, engine, if_exists='replace')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Call all functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping columns: ['genre', 'original', 'child_alone']\n"
     ]
    }
   ],
   "source": [
    "df_msg = load_messages(file_name='disaster_messages.csv')\n",
    "df_cat = load_categories(file_name='disaster_categories.csv')\n",
    "\n",
    "df_cat_exp = expand_categories(df_cat, column_name='categories')\n",
    "\n",
    "df_merge = merge_msg_cat(df_msg, df_cat_exp)\n",
    "\n",
    "df_clean = clean(df_merge,\n",
    "                 drop_columns=['genre', 'original', 'child_alone'],# Column child_alone is filled with zeros\n",
    "                 independent_vars=['message'], # The predictor variable\n",
    "                 replacements=[('related', (2,1))])# Replace 2 by 1 in the column 'related'\n",
    "\n",
    "save(df_clean, database='DisasterResponse.db', table_name='Messages')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
